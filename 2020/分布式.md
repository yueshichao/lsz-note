# CAP
- Consistency - 一致性
- Availability - 可用性
- Partition tolerance - 分区容错性

# [一致性Hash算法](https://juejin.im/post/6856278390107078663)

# 分布式锁
TODO

# 分布式事务
## 2PC - 2阶段提交
> 也有叫做XA模型的，但是XA全称是什么还不清楚
> 是一个强一致、中心化的原子提交协议

执行流程：
1. 请求表决
2. 执行/取消

## 3PC - 3阶段提交
> 相比于2PC，多了PreCommit，如果事务协调者（中心节点）挂了，其他参与者没有接收到Commit指令，参与者自动进行本地commit
1. CanCommit
2. PreCommit
3. DoCommit

## TCC - Try Confirm Cancel
> 2PC在应用层的实现
### [seata](https://github.com/seata/seata)
- 三个角色
  - TC - Transaction Coordinator
  - TM - Transaction Manager
  - RM - Resource Manager

- seata的生命周期
  1. TM发起全局事务，向TC要一个全局的XID（全局事务的ID）
  2. XID随微服务链路传播
  3. RM通过拿到的XID，将自己本地事务注册为（全局事务）对应的分支事务
  4. TM要求TC提交或回滚全局事务
  5. TC驱动所有的分支事务提交或回滚

## Paxos
> 参考：  
[微信自研生产级paxos类库PhxPaxos实现原理介绍](https://mp.weixin.qq.com/s?__biz=MzI4NDMyNTU2Mw==&mid=2247483695&idx=1&sn=91ea422913fc62579e020e941d1d059e#rd)  
[Paxos理论介绍(1): 朴素Paxos算法理论推导与证明](https://zhuanlan.zhihu.com/p/21438357)  
[朴素Paxos（Basic Paxos）算法java简易实现](https://my.oschina.net/u/2541538/blog/807185)  
[豆瓣 paxos 算法的理解](https://www.douban.com/note/525598913/)  
[Paxos算法详解](https://zhuanlan.zhihu.com/p/31780743)  
[分布式系列文章——Paxos算法原理与推导](https://www.cnblogs.com/linbingdong/p/6253479.html)  
[如何浅显易懂地解说 Paxos 的算法？](https://www.zhihu.com/question/19787937)  

### Basic Paxis
TODO


## Zookeeper
> 参考：  
[github qiurunze123/zookeeperDesign](https://github.com/qiurunze123/zookeeperDesign)
[ZooKeeper Java Example](https://zookeeper.apache.org/doc/current/javaExample.html#sc_completeSourceCode)

- 服务于分布式系统，对节点统一管理
- Zookeeper维护一棵树，节点叫做ZNode，分为短暂型和持久型
- C/S模式，客户端监听ZNode节点变化
- 可以做到`统一配置管理`、`统一命名服务`、`分布式锁`、`集群管理`

[如何实现统一配置](https://blog.csdn.net/u011320740/article/details/78742625)

### 启动连接命令
- bin/zkServer.sh start # 启动服务端
- bin/zkCli.sh -server localhost:2181 # 通过客户端脚本连接到服务端

### 交互命令
进入zkCli的交互界面后
- close # 关闭
- create /lsz "node data" # 在根节点下创建`lsz`节点，默认为持久型
  - create -s /lsz # 创建持久序号节点，s表示SEQUENTIAL，会返回实际路径 -> *Created /lsz0000000012*
  - create -e /lsz # 创建临时节点，e表示EPHEMERAL，断开会话后，该节点被自动删除
  - create -e -s /lsz # 创建临时序号节点
- get /lsz # 获取节点内容
- set /lsz "changed data" # 修改
- stat /lsz # 查看节点信息
- ls -w /lsz # 监听节点增删变化（怎么监听增加？）
- get -w /lsz # 监听节点内容变化
- stat -w /lsz # 监听状态变化

### ACL(Access Control List)
TODO

### 分布式锁思路
0. 创建锁节点父节点，如/lock
1. 每次**lock操作**创建临时序号节点node
2. 查询/lock下所有节点，找到node之前的节点，如果存在，则监听此节点
3. 如果不存在或被删除，则拿到锁
4. **unlock操作**，删除node

#### 使用curator完成分布式锁
> curator封装了许多操作，比如分布式锁  

1. 导包  
```xml
<!-- zookeeper客户端 -->
<!-- 对zookeeper的底层api的一些封装 -->
<dependency>
    <groupId>org.apache.curator</groupId>
    <artifactId>curator-framework</artifactId>
    <version>2.12.0</version>
</dependency>
<!-- 封装了一些高级特性，如：Cache事件监听、选举、分布式锁、分布式Barrier -->
<dependency>
    <groupId>org.apache.curator</groupId>
    <artifactId>curator-recipes</artifactId>
    <version>2.12.0</version>
</dependency>
```

2. 使用
```java
ExponentialBackoffRetry retryPolicy = new ExponentialBackoffRetry(1000, 3);
CuratorFramework client = CuratorFrameworkFactory.builder()
        .connectString(ZK_URL)
        .sessionTimeoutMs(5000)
        .connectionTimeoutMs(5000)
        .retryPolicy(retryPolicy)
        .namespace("curator_lock")
        .build();
client.start();

InterProcessMutex mutex = new InterProcessMutex(client, "/lockNode");
mutex.acquire();
try {
    log.info("acquire & do sth.");
    System.out.println("#_#");
} catch (Exception e) {
    e.printStackTrace();
} finally {
    mutex.release();
}
```

